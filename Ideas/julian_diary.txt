## data resilience

capability of the data for grow from its own ashes

## damage

is the damage suffered by the data in a process

it deppends on the shape of the data, sqared datasets are more likely to die


## Watson-Nadaraya estimation and distance prediction


The Nadaraya Watson Estimator is one of the most powerful regression tools. It aims to compute the conditional mean of a conjoint density kernel estimation. Generally, it can be named Nonparemetric Kernel Regression or Parzen Window Method and presented by this formula:

$$ \widehat{h}(x) = \sum\limits_{i = 1}^n w_iy_i$$

when $w_i = \frac{K\left(\frac{x_i - x}{b}\right)}{\sum\limits_{j = 1}^n K\left(\frac{x_j - x}{b}\right)}

In this way it has to hyperparameters which can have an impact on the estimator. The First hyperparameter to choose is the kernel $K$. For this is necessary to consider the type of variable, the range of the data, and the shape of the estimated density. When selected properly the kernel will produce a good approximation to the theoretical density of the data. When the gaussian kernel is used the estimation approach can be reduced to a distance process. 

The second hyperparameter to choose is the bandwidth $b$. The bandwidth will set the relevance of each point of data in the estimation process. So that, when it is near to zero the regresi√≥n will be likely to have an overfitting problem, because the regression function will be close to each point on the sample. When the bandwidth is good, the regression function will consider the general trend instead each point location. A too large band will cause the regression function will be strength horizontal curve.

In Practical terms the regression function will be the weighted average of the data. In the gaussian kernel case, the weights can be computed as the exponential of the negative of the distance to each data point. The multivariate implementation of this estimation process for a complete table is not complicated. That is:

$$ \widehat{h}(x) = \sum\limits_{i = 1}^n w_iy_i$$

when $w_i = \frac{exp\left( - \frac{\|x_i - x\|}{b}\right)}{\sum\limits_{j = 1}^n exp\left( - \frac{\|x_j - x\|}{b}\right)}$

Intuitively, the $w_i$ weights can be understod as scaled similarities. It means, given a new observation $x^*$ it's predicted value $x^*$ is calculated as a weighted average of the observed values of the variable response $ \sum_{i = 1}^n w_iy_i$. This weights sums one and reach higher values on observations $x_i$ closer to $x^*$. 

That distance based approach agrees with the distance based definition of function continuity. This is, a function $f: \mathbb{R}^m \mapsto \mathbb{R}^n$ is a continuous function if the distance between $f(x_1)$ and $f(x_2)$ decreases to 0 when the distance between $x_1$ and $x_2$ decreases to 0. So, when $f(x_2)$ is not known it can be appriximated by values of $f(x_1)$ for values of $x_1$ close to $x_2$ (or, in the wieghted average case, giving them high weights).

So, it's theoretically possible have a distance based approach to the prediction problem, and the Nadaraya-Watson estimation process is an excellent example of this. By mean this intuitive argumentation one can see the architecture of distance distance prediction. It can be summarized like

 - Scale the data. Because distances and similarities can be affected by different variable scales.
 - Calculate the distance between the new observation and all sample observations. 
 - Using a decreasing function transform the distance measure in a similatiry measure. Or, if possible, calculate a similarity measure between the new observation and all sample observations.
 - There is an hyper parameter called bandwidth which the distance is divided by. It has been studied yet and a large ammount of literature about it is provided. One can set this hyperparameter at one (1) by doing nothing to the similarity matrix although this behavior is discouraged because increases it can increase the overfitting or bias of the prediction.
 - Scaling the similarities to sum one will product the data weights. So, there is one weight for each pair of new-old observations, and the seights corresponding at a new observation must be all positive and sum one.
 - For each new observation compute the weighted average of the response ariable using its weights. This weighted average will be the prediction at this point.

Seeing the architecture and forgetting the details for a moment it's possible imagine it as a set of modules and change some of them. Some questions will be, 

 - What is the best way of scale the data in order to have better (most representative) distances?
 - What about use some disctinct distance function?
 - What about use some disctinct decreasing function?
 - What about use some non-distance-based similarity?
 - What about bandwith calculation for each variable (local bandwith)?
 - Is there a way to give some relevancy to the variables in distance calculation?
 - And so on...

In conclusion, here is a complete framework of distance based prediction. There are some procedures like parzen window, k nearest neighbors, hot deck, and many others, which are different approaches and implementations of this one paradigm. This paper aims combine the distance based prediction paradign with the EM algorithm in order to managing missing values.

## EM algorithm

The EM algorithm is an iterative likelihood maximization technique, fraamed in the classical parametric paradigm. Mostly, It is used in two different ways, to estimate the model parameters in presence of missing data, being an useful imputation technique, and managing models with hidden parameters. These algorithm is shaped by two steps. The E step: prediction of hidden parameters or missing data as the expectation of the conditional distribution, and the M step: estimation of the (explicit) parameters of the model via likelihood maximization. 

On the expectation step, having some values for the parameters in the model the missing data values (or the hidden parameter values) are predicted from the model as the expectation of their conditional distribution given the present data and the parameters. On the maximization step, having  some values for the missing data (or the hidden parameters), the (explicit) parameters are estimated via maximum likelihood. In this sense, it's required use a model with an easy parameter estimation process, because it will be repeated few times. Both steps are repeated until convergence. The mathematical explanation are available in [citation].

## Data damage 

In the imputation data context it's necessary to talk about the nature and amount of missing information. In this analysis some concepts like _missing at random_ and _missing completely at random_ show up, concepts that care about of the distribution of the damage from the data. But in the consulted papers were not deeper analysis of this damage. There are no standardized measures for this data damage or further descriptions  easy/difficult imputation. It means that its not able damage different data sets in the same way, or damage them in the same amount, for test imputation data algorithms.

Traditional approach for this problem is measure the data damage by the percent of missing values on a table. But this approach ignores the shape of the table, which will be relevant aspect show it in Figure 1.

The shape of the table is indeed relevant. Long vertical table will produce more fulfilled individuals than squared shaped table or fat horizontal table. This is important because every prediction algorithm is based (and improved) on complete cases. Intuitively, if every individual is missing out a few values, the task of imputation is harder.

In conclusion, the damage has different ways of perfect the data depending not only on its distribution but also on the firm of the table and other factors. Now, intiutively, samples of the same size of the same data table having the same missing values in the same spots predicted and filled by the same imputation methods will produce approximately the same level of affordability/confiability/prediction/accuracy. Tis property can be an initalization point for the search of data damage measuring.

It's required to pay attention to some concepts concerning the proportion and the quantity of fulfilled individuals, the shape of the table, the distribution of the missings on the table in order to have a measure that describes the damage weaselly/foley/perfectly/completely. So that, we can have the same amount of damage in different tables in order to evaluate our imputation algorithms

How can we Define and design with this measure. For this is necessary to status of statements that needs to be complete by the proposal measure. These statements are called requirements and determinants of the measure, and they will be:

 - Two data tables with the same size and the same shape having the same missing values in the same spot, independently of the data and the type of data, should have the same values of these measure.
 - Two tables with with the same size and the same shape  and the same values of this measure should have approximately the same number or fulfilled individuals.
 - Two data tables with same values of this measure should have the same proportion of fulfilled individuals.
 - Two samples of the same datatable the same data damage imputed with the same imputation methods should have this same imputation performance.

The individual distribution of the count of missings in the missing complete at random (MCAR) case is binomial. When the misings are randomly located in table, each cell has a non-zero probability of have a missing value, setting a bernoully scenary. The individual count of missing values is, in fact, a sum of Bernoully distributed random variables, so it is a binomial deistributed count whose parameters are the number of colums of table $k$ and the probability of have a missing $p$. The estimation of theese parameters is not difficult, the first parameter $k$ needs no estimation and having $n$ individuals and $m$ missings $\widehat{p} = \frac{m}{nk}$. The espected value of this binomial distribution will be $kp$ whose estimation is $\frac{m}{n}$. 

Having this in consideration we now discuss if this binomial distribution or its expectation satisfies the previous requirement statements in order to propose meassure the data damage by means this parameters and expectation.

## Data resilience

Data resilience is the property of data sets. Let's say the A data set is more resilient than other data set B, if after make them the same amount of damage in the same way, A can be recovered better than B using the same method of data imputation.

This definition is a sketch, because recovering different data sets, different imputation methods can lead to diferent results because their inner performance and architecture (linear prediction, distance based imputation, bayesian approach). So that, start the research about this concept is required in order to evaluate the accuracy or misconception of this definition.

@book{EM,
abstract = {The only single-source--now completely updated and revised--to offer a unified treatment of the theory, methodology, and applications of the EM algorithm Complete with updates that capture developments from the past decade, The EM Algorithm and Extensions, Second Edition successfully provides a basic understanding of the EM algorithm by describing its inception, implementation, and applicability in numerous statistical contexts. In conjunction with the fundamentals of the topic, the authors discuss convergence issues and computation of standard errors, and, in addition, unveil many parallels and connections between the EM algorithm and Markov chain Monte Carlo algorithms. Thorough discussions on the complexities and drawbacks that arise from the basic EM algorithm, such as slow convergence and lack of an in-built procedure to compute the covariance matrix of parameter estimates, are also presented. While the general philosophy of the First Edition has been maintained, this timely new edition has been updated, revised, and expanded to include: The EM Algorithm and Extensions, Second Edition serves as an excellent text for graduate-level statistics students and is also a comprehensive resource for theoreticians, practitioners, and researchers in the social and physical sciences who would like to extend their knowledge of the EM algorithm.},
author = {Mclachlan, Geoffrey J and Krishnan, Thriyambakam},
doi = {10.1002/9780470191613},
isbn = {9780471201700},
title = {{The EM Algorithm and Extensions Second Edition}},
year = {2008}
}

@article{bilmes1998gentle,
  title={A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models},
  author={Bilmes, Jeff A and others},
  journal={International Computer Science Institute},
  volume={4},
  number={510},
  pages={126},
  year={1998}
}

@article{schafer2002missing,
  title={Missing data: our view of the state of the art.},
  author={Schafer, Joseph L and Graham, John W},
  journal={Psychological methods},
  volume={7},
  number={2},
  pages={147},
  year={2002},
  publisher={American Psychological Association}
}
